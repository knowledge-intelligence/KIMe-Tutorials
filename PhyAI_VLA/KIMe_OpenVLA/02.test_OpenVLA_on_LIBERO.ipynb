{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet mediapy\n",
    "# !pip install robosuite==1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 14:31:31.052618: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-20 14:31:31.099393: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-20 14:31:31.099433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-20 14:31:31.100802: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-20 14:31:31.109809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-20 14:31:31.969650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">05/20 [14:31:50] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; No private macro file found!                                            <a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">macros.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#53\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m05/20 [14:31:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> No private macro file found!                                            \u001b]8;id=210166;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\u001b\\\u001b[2mmacros.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=547967;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#53\u001b\\\u001b[2m53\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; It is recommended to use a private macro file                           <a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">macros.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#54\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> It is recommended to use a private macro file                           \u001b]8;id=723644;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\u001b\\\u001b[2mmacros.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=360785;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#54\u001b\\\u001b[2m54\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> | &gt;&gt; To setup, run: python                                                   <a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">macros.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/sc</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">ripts/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">setup_macros.py</span>                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m | >> To setup, run: python                                                   \u001b]8;id=309887;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py\u001b\\\u001b[2mmacros.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=48896;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/macros.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/robosuite/sc\u001b[0m \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35mripts/\u001b[0m\u001b[95msetup_macros.py\u001b[0m                                                        \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; No OpenGL_accelerate module loaded: No module named          <a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/OpenGL/acceleratesupport.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">acceleratesupport.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/OpenGL/acceleratesupport.py#24\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'OpenGL_accelerate'</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> No OpenGL_accelerate module loaded: No module named          \u001b]8;id=942752;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/OpenGL/acceleratesupport.py\u001b\\\u001b[2macceleratesupport.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=312935;file:///home/kime/miniconda3/envs/openvla/lib/python3.10/site-packages/OpenGL/acceleratesupport.py#24\u001b\\\u001b[2m24\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[32m'OpenGL_accelerate'\u001b[0m                                               \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from experiments.robot.robot_utils import (\n",
    "    DATE_TIME,\n",
    "    get_action,\n",
    "    get_model,\n",
    "    invert_gripper_action,\n",
    "    normalize_gripper_action,\n",
    "    set_seed_everywhere,\n",
    ")\n",
    "from experiments.robot.openvla_utils import get_processor\n",
    "\n",
    "from experiments.robot.libero.libero_utils import (\n",
    "    get_libero_image,\n",
    "    quat2axisangle,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from pathlib import Path\n",
    "import os \n",
    "import mediapy\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateConfig:\n",
    "    # fmt: off\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Model-specific parameters\n",
    "    #################################################################################################################\n",
    "    model_family: str = \"openvla\"                    # Model family\n",
    "    pretrained_checkpoint: Union[str, Path] = \"\"     # Pretrained checkpoint path\n",
    "    load_in_8bit: bool = False                       # (For OpenVLA only) Load with 8-bit quantization\n",
    "    load_in_4bit: bool = True                       # (For OpenVLA only) Load with 4-bit quantization\n",
    "\n",
    "    center_crop: bool = True                         # Center crop? (if trained w/ random crop image aug)\n",
    "\n",
    "    #################################################################################################################\n",
    "    # LIBERO environment-specific parameters\n",
    "    #################################################################################################################\n",
    "    task_suite_name: str = \"libero_spatial\"          # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90\n",
    "    num_steps_wait: int = 10                         # Number of steps to wait for objects to stabilize in sim\n",
    "    num_trials_per_task: int = 50                    # Number of rollouts per task\n",
    "\n",
    "    #################################################################################################################\n",
    "    # Utils\n",
    "    #################################################################################################################\n",
    "    run_id_note: Optional[str] = None                # Extra note to add in run ID for logging\n",
    "    local_log_dir: str = \"./experiments/logs\"        # Local directory for eval logs\n",
    "\n",
    "    use_wandb: bool = False                          # Whether to also log results in Weights & Biases\n",
    "    wandb_project: str = \"YOUR_WANDB_PROJECT\"        # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"YOUR_WANDB_ENTITY\"          # Name of entity to log under\n",
    "\n",
    "    seed: int = 0                                    # Random Seed (for reproducibility)\n",
    "\n",
    "    # fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GenerateConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"libero_spatial\", \"libero_object\", \"libero_goal\", \"libero_10\"]\n",
    "cfg.task_suite_name = \"libero_10\" # can also choose libero_spatial, libero_object, etc.\n",
    "cfg.pretrained_checkpoint = f\"openvla/openvla-7b-finetuned-libero-10\"\n",
    "cfg.load_in_4bit = True\n",
    "resize_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OpenVLA] Set action un-normalization key\n",
    "cfg.unnorm_key = cfg.task_suite_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed_everywhere(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Instantiating Pretrained VLA model\n",
      "[*] Loading in BF16 with Flash-Attention Enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No local dataset_statistics.json file found for current checkpoint.\n",
      "You can ignore this if you are loading the base VLA (i.e. not fine-tuned) checkpoint.Otherwise, you may run into errors when trying to call `predict_action()` due to an absent `unnorm_key`.\n",
      "Loaded model: <class 'transformers_modules.openvla.openvla-7b.31f090d05236101ebfc381b61c674dd4746d4ce0.modeling_prismatic.OpenVLAForActionPrediction'>\n"
     ]
    }
   ],
   "source": [
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OpenVLA] Check that the model contains the action un-normalization key\n",
    "if cfg.model_family == \"openvla\":\n",
    "    # In some cases, the key must be manually modified (e.g. after training on a modified version of the dataset\n",
    "    # with the suffix \"_no_noops\" in the dataset name)\n",
    "    if cfg.unnorm_key not in model.norm_stats and f\"{cfg.unnorm_key}_no_noops\" in model.norm_stats:\n",
    "        cfg.unnorm_key = f\"{cfg.unnorm_key}_no_noops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'libero_10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.unnorm_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OpenVLA] Get Hugging Face processor\n",
    "processor = None\n",
    "if cfg.model_family == \"openvla\":\n",
    "    processor = get_processor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libero.libero import benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "from libero.libero import get_libero_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_libero_env(task, resolution=256):\n",
    "    \"\"\"Initializes and returns the LIBERO environment, along with the task description.\"\"\"\n",
    "    task_description = task.language\n",
    "    task_bddl_file = os.path.join(get_libero_path(\"bddl_files\"), task.problem_folder, task.bddl_file)\n",
    "    env_args = {\"bddl_file_name\": task_bddl_file, \"camera_heights\": resolution, \"camera_widths\": resolution}\n",
    "    env = OffScreenRenderEnv(**env_args)\n",
    "    env.seed(0)  # IMPORTANT: seed seems to affect object positions even when using fixed initial state\n",
    "    return env, task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Task suite: libero_10\n"
     ]
    }
   ],
   "source": [
    "# Initialize LIBERO task suite\n",
    "benchmark_dict = benchmark.get_benchmark_dict()\n",
    "task_suite = benchmark_dict[cfg.task_suite_name]()\n",
    "num_tasks_in_suite = task_suite.n_tasks\n",
    "print(f\"Task suite: {cfg.task_suite_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning]: datasets path /home/kime/ws_openvla/LIBERO/libero/libero/../datasets does not exist!\n",
      "[Warning]: datasets path /home/kime/ws_openvla/LIBERO/libero/libero/../datasets does not exist!\n",
      "task_description:\n",
      "put the white mug on the plate and put the chocolate pudding to the right of the plate\n"
     ]
    }
   ],
   "source": [
    "# Get task\n",
    "task = task_suite.get_task(task_id)\n",
    "\n",
    "# Get default LIBERO initial states\n",
    "initial_states = task_suite.get_task_init_states(task_id)\n",
    "\n",
    "# Initialize LIBERO environment and task description\n",
    "env, task_description = get_libero_env(task, resolution=256)\n",
    "\n",
    "print('task_description:')\n",
    "print(task_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def save_rollout_video(rollout_images, idx):\n",
    "    \"\"\"Saves an MP4 replay of an episode.\"\"\"\n",
    "    mp4_path = f\"./demo_task_{idx}.mp4\"\n",
    "    video_writer = imageio.get_writer(mp4_path, fps=30)\n",
    "    for img in rollout_images:\n",
    "        video_writer.append_data(img)\n",
    "    video_writer.close()\n",
    "    print(f\"Saved rollout MP4 at path {mp4_path}\")\n",
    "    return mp4_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def test(task_successes, episode_idx, instruction):\n",
    "\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "\n",
    "    # Set initial states\n",
    "    obs = env.set_init_state(initial_states[episode_idx])\n",
    "\n",
    "    # Setup\n",
    "    t = 0\n",
    "    replay_images = []\n",
    "    if cfg.task_suite_name == \"libero_spatial\":\n",
    "        max_steps = 400*2  # longest training demo has 193 steps\n",
    "    elif cfg.task_suite_name == \"libero_object\":\n",
    "        max_steps = 400*2  # longest training demo has 254 steps\n",
    "    elif cfg.task_suite_name == \"libero_goal\":\n",
    "        max_steps = 400*2  # longest training demo has 270 steps\n",
    "    elif cfg.task_suite_name == \"libero_10\":\n",
    "        max_steps = 520*2  # longest training demo has 505 steps\n",
    "    elif cfg.task_suite_name == \"libero_90\":\n",
    "        max_steps = 400*2  # longest training demo has 373 steps\n",
    "\n",
    "    print(f\"Starting task : {task}...\")\n",
    "\n",
    "    sim_steps = list(range(max_steps + cfg.num_steps_wait))\n",
    "    for t in tqdm(sim_steps, desc=\"처리 중\"):\n",
    "        try:\n",
    "            # IMPORTANT: Do nothing for the first few timesteps because the simulator drops objects\n",
    "            # and we need to wait for them to fall\n",
    "            if t < cfg.num_steps_wait:\n",
    "                get_libero_dummy_action = [0, 0, 0, 0, 0, 0, -1]\n",
    "                obs, reward, done, info = env.step(get_libero_dummy_action)\n",
    "                continue\n",
    "\n",
    "            # Get preprocessed image\n",
    "            img = get_libero_image(obs, resize_size)\n",
    "\n",
    "            # Save preprocessed image for replay video\n",
    "            replay_images.append(img)            \n",
    "            # cv2.imshow(\"Real-time View\", cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Prepare observations dict\n",
    "            # Note: OpenVLA does not take proprio state as input\n",
    "            observation = {\n",
    "                \"full_image\": img,\n",
    "                \"state\": np.concatenate(\n",
    "                    (obs[\"robot0_eef_pos\"], quat2axisangle(obs[\"robot0_eef_quat\"]), obs[\"robot0_gripper_qpos\"])\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Query model to get action\n",
    "            action = get_action(\n",
    "                cfg,\n",
    "                model,\n",
    "                observation,\n",
    "                instruction,\n",
    "                processor=processor,\n",
    "            )\n",
    "\n",
    "            # Normalize gripper action [0,1] -> [-1,+1] because the environment expects the latter\n",
    "            action = normalize_gripper_action(action, binarize=True)\n",
    "\n",
    "            # [OpenVLA] The dataloader flips the sign of the gripper action to align with other datasets\n",
    "            # (0 = close, 1 = open), so flip it back (-1 = open, +1 = close) before executing the action\n",
    "            if cfg.model_family == \"openvla\":\n",
    "                action = invert_gripper_action(action)\n",
    "\n",
    "            # Execute action in environment\n",
    "            obs, reward, done, info = env.step(action.tolist())\n",
    "            if done:\n",
    "                task_successes += 1\n",
    "                print('Success', task_successes)\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Caught exception: {e}\")\n",
    "            break\n",
    "    \n",
    "    return replay_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTask: {task_description}\")\n",
    "\n",
    "task_successes = 0\n",
    "episode_idx = 0\n",
    "instruction = task_description\n",
    "\n",
    "replay_images = test(task_successes, episode_idx, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapy.show_video(replay_images, fps=15, codec='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapy.write_image('/tmp/checkerboard.gif', replay_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save a replay video of the episode\n",
    "# save_rollout_video(replay_images, task_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addtional Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task_successes = 0\n",
    "# episode_idx = 0\n",
    "# instruction = \"put the chocolate pudding on the plate\"\n",
    "\n",
    "# replay_images = test(task_successes, episode_idx, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapy.show_video(replay_images, fps=15, codec='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task_successes = 0\n",
    "# episode_idx = 0\n",
    "# instruction = \"put the chocolate pudding on the white mug\"\n",
    "\n",
    "# replay_images = test(task_successes, episode_idx, instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapy.show_video(replay_images, fps=15, codec='gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
