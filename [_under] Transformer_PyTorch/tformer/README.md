# Transformer-from-Scratch
Read my articles for a detailed explanation: [How Transformer Works](https://medium.com/@sayedebad.777/mastering-transformer-detailed-insights-into-each-block-and-their-math-4221c6ee0076) ;  [Build a Transformer from Scratch](https://medium.com/@sayedebad.777/mastering-transformer-detailed-insights-into-each-block-and-their-math-4221c6ee0076) and [Train a Transformer Model](https://medium.com/@sayedebad.777/training-a-transformer-model-from-scratch-25bb270f5888)

I have chosen the translation task (English to Italian) to train my Transformer model on the [opus_books](https://huggingface.co/datasets/Helsinki-NLP/opus_books) dataset from Hugging Face. The training of this model was done on Kaggle using an **NVIDIA Tesla P100 - 16GB GPU**. It took **5 hours and 11 minutes** for training over **20 epochs** and each epoch has **3638 batches** to train on.
